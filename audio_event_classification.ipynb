{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUlEYDnJhagv"
   },
   "source": [
    "# Audio Event Classification\n",
    "\n",
    "The aim of this project is to produce a working Neural Network model that can classify audio events, with particular regard to crime incidents.\n",
    "\n",
    "The datasets used to train the network were provided by the teacher. The [raw dataset](https://www.kaggle.com/datasets/afisarsy/raw-audio-of-accident-and-crime-detection) contains plain samples of audio events, while the [enhanced dataset](https://www.kaggle.com/datasets/afisarsy/enhanced-audio-of-accident-and-crime-detection) contains the same samples of the raw dataset, which have been mixed with noise effects (wind, thunderstorm, rain and road traffic).\n",
    "\n",
    "In total, the final model needs to distinguish between 13 different audio classes, which are the following:\n",
    "* car_crash\n",
    "* conversation\n",
    "* engine_idling\n",
    "* gun_shot\n",
    "* jambret\n",
    "* maling\n",
    "* rain\n",
    "* rampok\n",
    "* road_traffic\n",
    "* scream\n",
    "* thunderstorm\n",
    "* tolong\n",
    "* wind\n",
    "\n",
    "## Structure, Network and Framework\n",
    "The project was intended to be run as individual Python scripts, just like a normal program would be run with a series of parameters. For the purpose of the project's presentation, a Python Jupyter Notebook was assembled listing the content of each individual script.\n",
    "\n",
    "The network adopted is an LSTM RNN implemented using PyTorch.\n",
    "\n",
    "## Bibliography and external sources\n",
    "The majority of the project needed a particular regard with respect to the specific dataset used and available system resources. For that reason, most of the code is brand new and original. However, some parts of the project required the use of already existing code, in order to have a working basis for the other modules involved.\n",
    "\n",
    "In particular, the following sources were used for inspiration:\n",
    "* [seth814/Audio-Classification](https://github.com/seth814/Audio-Classification), for the structure of the LSTM network\n",
    "* [Audio Classification Starter](https://www.kaggle.com/code/aayush9753/audio-classification-starter-in-pytorch/notebook), for the use of Datasets and DataLoaders\n",
    "* [Audio Classification with LSTM](https://www.kaggle.com/code/kvpratama/audio-classification-with-lstm-and-torchaudio), for the training and testing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-YBhXW8gefU"
   },
   "source": [
    "# AudioEventDataset.py\n",
    "\n",
    "A Pytorch Dataset is a class used to handle the actual retrivial of data samples. It allows for a customizable processing of the data that can be adapted to one's specific needs.\n",
    "\n",
    "In our case, the AudioEventDataset class (which extends the generic Dataset parent class provided by Pytorch) takes in input a source directory, a frame size + hop size (in seconds) and a sampling rate.\n",
    "The actual method in charge of retrieving and pre-processing the audio samples is `__getitem__()`, which loads the audio file given a path, chunks it into several frames and returns a framed audio tensor with the correct label associated with it. The label is formatted as a *one-hot encoded* tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHhcGul5gaM5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class AudioEventDataset(Dataset):\n",
    "\n",
    "    def __init__(self, src_dir, frame_size, hop_size, sample_rate):\n",
    "        super().__init__()\n",
    "        self.src_dir = src_dir\n",
    "        self.frame_size = frame_size\n",
    "        self.hop_size = hop_size\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        audio_paths_absolute = [x for x in glob('{}/**'.format(src_dir), recursive = True) if '.wav' in x] # look for all files within src_dir\n",
    "        audio_paths = []\n",
    "        for path in audio_paths_absolute:\n",
    "            audio_paths.append(os.path.relpath(path, src_dir)) # append just the relative path\n",
    "\n",
    "        self.paths = audio_paths\n",
    "\n",
    "        self.classes = sorted(list(set( [x.split('/')[0] for x in self.paths] )))\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.classes) # use an encoder to convert labels to integers\n",
    "\n",
    "        print(f\"Total samples found: {len(self)}\\n\")\n",
    "        print(f\"Total classes found: {self.n_classes}\")\n",
    "        for class_item in self.classes:\n",
    "            print(f\"    - {class_item}\")\n",
    "        print('\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path = self.paths[index]\n",
    "\n",
    "        audio_data, _ = torchaudio.load(os.path.join(self.src_dir, path), normalize = True) # load and normalize the audio file\n",
    "        framed_audio_data = AudioEventDataset.frame_audio_overlap(audio_data, self.frame_size, self.sample_rate, self.hop_size) # chunk it into several frames\n",
    "        label = path.split('/')[0]\n",
    "\n",
    "        label = self.label_encoder.transform([label])[0]\n",
    "        labels = np.array([np.eye(self.n_classes, dtype = 'float')[label]]) # create an one-hot encoded label tensor\n",
    "\n",
    "        labels = np.repeat(labels, framed_audio_data.size()[0], axis = 0) # repeat it as many times as the frames created\n",
    "\n",
    "        return framed_audio_data, torch.tensor(labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def frame_audio_overlap(audio_tensor, frame_size, sample_rate, hop_size):\n",
    "        audio = audio_tensor[0]\n",
    "        frame_size_samples = int(frame_size * sample_rate)\n",
    "\n",
    "        if hop_size:\n",
    "            hop_size_samples = int(hop_size * sample_rate)\n",
    "        else: hop_size_samples = frame_size_samples\n",
    "\n",
    "        frames = torch.empty((0, frame_size_samples), dtype = torch.float32)\n",
    "\n",
    "        for start in range(0, audio.size()[0], hop_size_samples):\n",
    "            end = start + frame_size_samples\n",
    "            frame = audio[start:end]\n",
    "            if len(frame) < frame_size_samples:\n",
    "                if len(frame) >= (frame_size_samples / 2): # if the size of the remaining sample is not shorter than half the frame size...\n",
    "                    frame = nn.functional.pad(frame, (0, frame_size_samples - frame.size()[0]), 'constant', value = 0) # pad with zeros\n",
    "                else: break\n",
    "            frame = torch.reshape(frame, (1, -1)) # reshape as a batched sample\n",
    "            frames = torch.cat([frames, frame], dim = 0) # concatenate\n",
    "\n",
    "        return frames\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from glob import glob\n",
    "    import os\n",
    "    src_dir = '/home/ldomeneghetti/Documents/Forensics/audio_classification_pytorch/raw_audio/car_crash'\n",
    "    audio_paths_absolute = [x for x in glob('{}/**'.format(src_dir), recursive = True) if '.wav' in x]\n",
    "    audio_paths = []\n",
    "    for path in audio_paths_absolute:\n",
    "        audio_paths.append(os.path.relpath(path, src_dir))\n",
    "\n",
    "    dataset = AudioEventDataset(audio_paths, src_dir, 1, 0.5, 44100)\n",
    "\n",
    "    item = dataset.__getitem__(0)\n",
    "\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJDcqcAggrID"
   },
   "source": [
    "# models.py\n",
    "\n",
    "The LSTMNetwork class provides the functionality for the LSTM Neural Network model to be used in the project. First, each layer is defined and initialized by the `__init__()` method, then a `forward()` chain is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6zQXApVgu-l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import math\n",
    "from torchinfo import summary\n",
    "\n",
    "class LSTMNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, sample_rate = 44100, sample_duration = 1.0, print_summary = True):\n",
    "\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "\n",
    "        self.melspectrogram = transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            n_mels=128,\n",
    "            hop_length=256,\n",
    "            f_max= int(sample_rate / 2) # Shannon's sampling theorem, set max frequency equal to half sampling frequency\n",
    "        )\n",
    "        self.amplitude_to_db = transforms.AmplitudeToDB()\n",
    "\n",
    "        self.mfcc = transforms.MFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mfcc=40,\n",
    "            melkwargs={\"n_fft\": 2048, \"hop_length\": 256, \"n_mels\": 128, \"f_max\": int(sample_rate / 2)}\n",
    "        )\n",
    "\n",
    "        self.layer_norm_mel = nn.LayerNorm(128)\n",
    "        self.layer_norm_mfcc = nn.LayerNorm(40)\n",
    "\n",
    "        self.bidirectional_lstm_mel = nn.LSTM(\n",
    "            input_size = 128, hidden_size = 256, num_layers = 1, batch_first = True, bidirectional = True\n",
    "        )\n",
    "\n",
    "        self.bidirectional_lstm_mfcc = nn.LSTM(\n",
    "            input_size = 40, hidden_size = 256, num_layers = 1, batch_first = True, bidirectional = True\n",
    "        )\n",
    "\n",
    "        self.dense_1_relu = nn.Linear(1024, 512)  # LSTM outputs (512 + 512)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(512) # batch normalization to avoid overfitting\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5) # dropout layer to avoid overfitting\n",
    "\n",
    "        self.dense_2_relu = nn.Linear(512, 128)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(128)\n",
    "        self.dense_3_relu = nn.Linear(128, 64)\n",
    "        self.batch_norm_3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, n_classes)\n",
    "\n",
    "        if(print_summary): summary(self, torch.Size([1, 44100]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mel = self.melspectrogram(x)\n",
    "        x_mel = self.amplitude_to_db(x_mel)\n",
    "\n",
    "        x_mfcc = self.mfcc(x)\n",
    "\n",
    "        x_mel = x_mel.permute(0, 2, 1)\n",
    "        x_mel = self.layer_norm_mel(x_mel)\n",
    "\n",
    "        x_mfcc = x_mfcc.permute(0, 2, 1)\n",
    "        x_mfcc = self.layer_norm_mfcc(x_mfcc)\n",
    "\n",
    "        lstm_out_mel, (h_n_mel, _) = self.bidirectional_lstm_mel(x_mel)\n",
    "        lstm_out_mfcc, (h_n_mfcc, _) = self.bidirectional_lstm_mfcc(x_mfcc)\n",
    "\n",
    "        x_mel = torch.cat([h_n_mel[-2,:,:], h_n_mel[-1,:,:]], dim=1)\n",
    "        x_mfcc = torch.cat([h_n_mfcc[-2,:,:], h_n_mfcc[-1,:,:]], dim=1) # get the last two hidden layers, both forward and reverse\n",
    "\n",
    "        x = torch.cat([x_mel, x_mfcc], dim=1) # concatenate Mel and MFCC representation\n",
    "\n",
    "        x = self.dense_1_relu(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dense_2_relu(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense_3_relu(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits # return logits without softmax (CrossEntropyLoss will be used later)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    lstm = LSTMNetwork(10, 44100, 1.0).to(device)\n",
    "    summary(lstm, torch.Size([1, 44100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJCUwyKfg1fM"
   },
   "source": [
    "# train.py\n",
    "\n",
    "The train module consists of two main steps:\n",
    "*   initialize the data and prepare it to be fed to the LSTM network\n",
    "*   the actual training + testing of the model\n",
    "\n",
    "The actual `train()` function encapsulates the `train_one_epoch()` function which performs the training for a single epoch; iterating the latter, we obtain a full training cycle on the network. The training is performed on the GPU.\n",
    "\n",
    "After the training, a single cycle of unbatched testing is performed on the CPU (due to insufficient memory on the host device). The testing allows to produce a confusion matrix which is then printed and can be saved.\n",
    "\n",
    "After the whole process of training + testing, the model is saved both in a complete format and in a reduced format (weights only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsUSOP6Fg3N8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AudioEventDataset import AudioEventDataset\n",
    "from models import LSTMNetwork\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "def collate_audio(batch): # function needed to properly concatenate audio frames into a single batch\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for element in batch:\n",
    "        data_list.append(element[0])\n",
    "        label_list.append(element[1])\n",
    "\n",
    "    data_tensor = torch.cat(data_list, dim = 0)\n",
    "    label_tensor = torch.cat(label_list, dim = 0)\n",
    "\n",
    "    return data_tensor, label_tensor\n",
    "\n",
    "\n",
    "def initialize_train(args):\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"QT_QPA_PLATFORM\"] = \"xcb\"\n",
    "\n",
    "    src_dir = args.src_dir\n",
    "    batch_size = args.batch_size\n",
    "    frame_size = args.frame_size\n",
    "    hop_size = args.hop_size\n",
    "    sample_rate = args.sample_rate\n",
    "    test_size = args.test_size\n",
    "    random_state = args.random_state\n",
    "    epochs = args.epochs\n",
    "    out_file = args.out_file\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    audio_dataset = AudioEventDataset(src_dir, frame_size, hop_size, sample_rate)\n",
    "\n",
    "    assert (test_size > 0 and test_size < 1), \"test_size must be strictly between 0 and 1\"\n",
    "    train_dataset, test_dataset = random_split(audio_dataset, [1.0 - test_size, test_size], generator = torch.Generator().manual_seed(random_state))\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_audio)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = True, collate_fn = collate_audio)\n",
    "\n",
    "    with open('classes.json', 'w', encoding = 'utf-8') as json_file:\n",
    "        json.dump({'classes': audio_dataset.classes}, json_file, ensure_ascii = False, indent = 4) # save classes' labels in a JSON file\n",
    "\n",
    "    lstm_model = LSTMNetwork(audio_dataset.n_classes, sample_rate, frame_size).to(device)\n",
    "\n",
    "    train(lstm_model, train_dataloader, test_dataloader, epochs, device, audio_dataset.classes)\n",
    "\n",
    "    torch.save(lstm_model.state_dict(), out_file + \"_dict.pth\") # save the model's dictionary (just the weights)\n",
    "    torch.save(lstm_model, out_file + \".pth\") # save the whole model\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dl, test_dl, epochs, device, classes):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                    max_lr = 0.001,\n",
    "                                                    steps_per_epoch = int(len(train_dl)),\n",
    "                                                    epochs = epochs,\n",
    "                                                    anneal_strategy = 'linear')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_dl, criterion, optimizer, device) # train the model for one epoch, return train loss and accuracy\n",
    "\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f}        Train Acc:  {train_acc:.4f}\")\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    test_loss, test_acc = test_model(model, test_dl, criterion, classes) # after training, test the model and return test loss and test accuracy\n",
    "    print(\"\\nFinal model:\")\n",
    "    print(f\"    Train Loss: {train_loss:.4f}        Train Acc:  {train_acc:.4f}\")\n",
    "    print(f\"    Test Loss: {test_loss:.4f}        Test Acc:  {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dl, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_data = 0\n",
    "\n",
    "    progress_bar = tqdm(dl, desc = \"Training\", unit = \"batch\")\n",
    "\n",
    "    for data, target in progress_bar:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target) # compute the loss\n",
    "        loss.backward() # backpropagate the loss\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output, dim = 1)\n",
    "        _, correct = torch.max(target, dim = 1)\n",
    "\n",
    "        total_data += data.size(0)\n",
    "\n",
    "        partial_correct = torch.sum(predicted == correct).item()\n",
    "        partial_accuracy = partial_correct / data.size(0)\n",
    "        partial_loss = loss.item()\n",
    "\n",
    "        total_correct += partial_correct\n",
    "        total_loss += partial_loss * data.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\"Accuracy\": partial_accuracy, \"Loss\": partial_loss})\n",
    "\n",
    "    epoch_loss = total_loss / total_data\n",
    "    epoch_acc = total_correct / total_data\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, dl, criterion, classes):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_data = 0\n",
    "\n",
    "    progress_bar = tqdm(dl, desc = \"Testing\", unit = \"batch\")\n",
    "\n",
    "    correct_labels = torch.empty((0), dtype = int)\n",
    "    predicted_labels = torch.empty((0), dtype = int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (data, target) in enumerate(dl):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            _, predicted = torch.max(output, dim = 1)\n",
    "            _, correct = torch.max(target, dim = 1)\n",
    "\n",
    "            correct_labels = torch.cat([correct_labels, correct])\n",
    "            predicted_labels = torch.cat([predicted_labels, predicted])\n",
    "\n",
    "            total_data += data.size(0)\n",
    "\n",
    "            partial_correct = torch.sum(predicted == correct).item()\n",
    "            partial_accuracy = partial_correct / data.size(0)\n",
    "            partial_loss = loss.item()\n",
    "\n",
    "            total_correct += partial_correct\n",
    "            total_loss += partial_loss * data.size(0)\n",
    "\n",
    "            progress_bar.set_postfix({\"Accuracy\": partial_accuracy, \"Loss\": partial_loss})\n",
    "\n",
    "    display = ConfusionMatrixDisplay.from_predictions(correct_labels.tolist(), predicted_labels.tolist(), display_labels = classes, xticks_rotation = 'vertical')\n",
    "    display.plot()\n",
    "    plt.show() # compute and plot the confusion matrix\n",
    "\n",
    "    test_loss = total_loss / total_data\n",
    "    test_acc = total_correct / total_data\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description = \"Audio Event Classification training\")\n",
    "\n",
    "    parser.add_argument('--src-dir', type = str, required = True, help = \"directory of source audio files\")\n",
    "\n",
    "    parser.add_argument('--batch-size', type = int, default = 16, help = \"batch size\")\n",
    "\n",
    "    parser.add_argument('--frame-size', type = float, default = 1.0, help = \"audio frame size in seconds\")\n",
    "\n",
    "    parser.add_argument('--hop-size', type = float, default = 0.5, help = \"audio hop size in seconds\")\n",
    "\n",
    "    parser.add_argument('--sample-rate', type = int, default = 44100, help = \"sample rate\")\n",
    "\n",
    "    parser.add_argument('--test-size', type = float, default = 0.1, help = \"ammount of samples (from 0.0 to 1.0) to be used for testing\")\n",
    "\n",
    "    parser.add_argument('--random-state', type = int, default = 2159017, help = \"random state for samples shuffling\")\n",
    "\n",
    "    parser.add_argument('--epochs', type = int, default = 16, help = \"epochs to train the network\")\n",
    "\n",
    "    parser.add_argument('--out-file', type = str, default = \"best_model\", help = \"name for the output model file\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    initialize_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6yALYVtg_pf"
   },
   "source": [
    "# predict.py\n",
    "\n",
    "After having trained the model, predictions can be performed starting from new, unseen audio files.\n",
    "\n",
    "The prediction follows a similar fashion to the pre-processing used during training: the audio data is divided into smaller windows (each window with a partial overlap with the preceeding and following), and prediction is performed on each of those windows.\n",
    "\n",
    "Using some tollerance parameters, an *event segments array* is produced, which is a list of all the events occuring in the audio including the starting instant and the ending instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytRAt2PlhBJr"
   },
   "outputs": [],
   "source": [
    "from AudioEventDataset import AudioEventDataset\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from models import LSTMNetwork\n",
    "\n",
    "import argparse\n",
    "\n",
    "import json\n",
    "\n",
    "import math\n",
    "\n",
    "import pygame\n",
    "\n",
    "def initialize_predict(args):\n",
    "\n",
    "    audio_file = args.audio_file\n",
    "    sample_rate = args.sample_rate\n",
    "    win_size = args.win_size\n",
    "    win_hop = args.win_hop\n",
    "    win_min = args.win_min\n",
    "    threshold = args.threshold\n",
    "    model_file = args.model_file\n",
    "    classes_file = args.classes_file\n",
    "\n",
    "    with open(classes_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    classes = data['classes'] # import the classes' labels from the JSON file previously saved\n",
    "\n",
    "    audio_data, _ = torchaudio.load(audio_file, normalize = True) # load the given audio data\n",
    "    play_audio(audio_file) # play the audio\n",
    "    processed_audio = AudioEventDataset.frame_audio_overlap(audio_data, win_size, sample_rate, win_hop) # chunk the audio into equally sized frames\n",
    "\n",
    "    model = torch.load(model_file, weights_only = False) # load the model\n",
    "\n",
    "    predict(model, processed_audio, classes, threshold, sample_rate, win_size, win_hop, win_min) # perform prediction\n",
    "\n",
    "\n",
    "def predict(model, input, classes, threshold, sample_rate, win_size, win_hop, min_frames):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    input = input.to('cpu')\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    output_sigmoid = torch.nn.Sigmoid()(output) # convert logits to Sigmoid\n",
    "    output_normalized = torch.nn.functional.normalize(output_sigmoid) # normalize Sigmoid\n",
    "\n",
    "    predicted_classes = []\n",
    "    for tensor in output_normalized:\n",
    "        value, index = torch.max(tensor, dim = 0)\n",
    "        if value.item() >= threshold:\n",
    "            predicted_classes.append(index.item())\n",
    "        else: predicted_classes.append(-1)\n",
    "\n",
    "    detected_events = group_contiguous(predicted_classes, min_frames, win_size, win_hop) # group contiguous events\n",
    "\n",
    "    # print event segments\n",
    "    print(\"\\n##################### DETECTED EVENTS #####################\")\n",
    "    for event in detected_events:\n",
    "        start_sec = event[1]*win_hop\n",
    "        end_sec = (event[2])*win_hop + win_size\n",
    "        print(f\"    - {classes[event[0]]}: start {start_sec:.2f} sec  /  end {end_sec:.2f} sec\")\n",
    "    print(\"###########################################################\\n\")\n",
    "\n",
    "\n",
    "def group_contiguous(input_list, min_contiguous, win_size, win_hop):\n",
    "    grouped_list = []\n",
    "    previous_item = input_list[0]\n",
    "\n",
    "    count = 0\n",
    "    start_index = 0\n",
    "    end_index = 1\n",
    "    for index, item in enumerate(input_list):\n",
    "        if item != previous_item:\n",
    "            if previous_item != -1 and count >= min_contiguous:\n",
    "                end_index = start_index + count - 1\n",
    "                grouped_list.append([previous_item, start_index, end_index])\n",
    "            start_index = index\n",
    "            previous_item = item\n",
    "            count = 1\n",
    "        else: count += 1\n",
    "\n",
    "    i = 1\n",
    "    end = len(grouped_list)\n",
    "    while i < end:\n",
    "        if grouped_list[i][0] == grouped_list[i-1][0]:\n",
    "            next_start = grouped_list[i][1]\n",
    "            previous_end = grouped_list[i-1][2]\n",
    "            if next_start - previous_end <= math.floor(win_size/win_hop):\n",
    "                grouped_list[i][1] = grouped_list[i-1][1]\n",
    "                del grouped_list[i-1]\n",
    "                end -= 1\n",
    "        i += 1\n",
    "\n",
    "    return grouped_list\n",
    "\n",
    "def play_audio(filename):\n",
    "\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description = \"Audio Event Classification prediction\")\n",
    "\n",
    "    parser.add_argument('--audio-file', type = str, required = True, help = 'source audio file')\n",
    "\n",
    "    parser.add_argument('--sample-rate', type = int, default = 44100, help = 'sample rate')\n",
    "\n",
    "    parser.add_argument('--win-size', type = float, default = 0.4, help = 'detection window length')\n",
    "\n",
    "    parser.add_argument('--win-hop', type = float, default = 0.2, help = 'detection window hop length')\n",
    "\n",
    "    parser.add_argument('--win-min', type = int, default = 2, help = 'minimum contiguous windows required to detect an event')\n",
    "\n",
    "    parser.add_argument('--threshold', type = float, default = 0.8, help = 'value above which a prediction is considered valid')\n",
    "\n",
    "    parser.add_argument('--model-file', type = str, default = 'lstm_network.pth', help = 'filename of the model to be loaded')\n",
    "\n",
    "    parser.add_argument('--classes-file', type = str, default = 'classes.json', help = 'filename of the classes file')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    initialize_predict(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
